{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problema MNIST"
      ],
      "metadata": {
        "id": "gjLb-yR71MVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O Problema MNIST e a Solução com InceptionV3\n",
        "### Introdução\n",
        "Como programador de deep learning, minha tarefa é resolver problemas complexos de reconhecimento de padrões e imagens. Vou explicar o problema do MNIST e como podemos resolvê-lo utilizando o modelo InceptionV3.\n",
        "\n",
        "### O Problema MNIST\n",
        "O conjunto de dados MNIST (Modified National Institute of Standards and Technology) é um benchmark clássico no campo de aprendizado de máquina. Ele consiste em 60.000 imagens de treino e 10.000 imagens de teste de dígitos manuscritos (0-9), cada uma em uma matriz de 28x28 pixels. O objetivo é classificar corretamente esses dígitos com um modelo de reconhecimento de padrões. Este problema é comumente utilizado para testar e comparar a eficácia de diferentes algoritmos de aprendizado de máquina.\n",
        "\n",
        "https://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "### Descrição do Modelo InceptionV3\n",
        "O InceptionV3 é uma arquitetura de rede neural convolucional avançada desenvolvida pelo Google. Ele é conhecido por sua eficiência e precisão em tarefas de classificação de imagens. O modelo é composto por vários blocos de camadas convolucionais que permitem a extração de características a diferentes escalas, tornando-o robusto para a detecção de padrões complexos.\n",
        "\n",
        "https://keras.io/api/applications/inceptionv3/"
      ],
      "metadata": {
        "id": "8zngHTiUBoxU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pCcNsBlb0Vs2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() # definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # Carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) # Carrega a parte de validação do dataset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "5FwDWIb61gEa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conferindo se a estrutura de dados está representando a imagem corretamente\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = next(dataiter)\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "N40Yl-nC2qBm",
        "outputId": "77f586b4-51e1-4229-d10d-6a789cdde1c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ecfaea64730>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificaras dimensões do tensor para img e etiqueta\n",
        "\n",
        "print(imagens[0].shape)\n",
        "print(etiquetas[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTtTh1TD4awC",
        "outputId": "ae75cf6c-03e5-4bb7-c86f-5ee6061bc1b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estrutura da rede InceptionV3 - Keras\n",
        "\n",
        "class Modelo(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) # layer de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) # layer intermédio, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) # layer de saída, 64 neurônios que se ligam a 10\n",
        "    # para a camada de saída não é necessário definir nada pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X)) # função de ativação ReLU da camada de entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) # função de ativação ReLU da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) # Não precisamos de uma função de ativação na camada de saída pois ela é uma regressão linear\n",
        "    return F.log_softmax(X, dim=1) # aplica a função de log_softmax para calcular a perda"
      ],
      "metadata": {
        "id": "wF-fhV7p4fpr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Otimizador\n",
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # otimizador estocástico gradiente, define a política de atualizaação dos pesos e da bias\n",
        "  inicio = time() # marca o tempo de início do treinamento\n",
        "\n",
        "  criterio = nn.NLLLoss() # função de perda\n",
        "  EPOCHS = 10 # número de epochs que o algoritmo rodará\n",
        "  modelo.train() # ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # variável para acumular a perda\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # redimensionando as imagens, convertendo as imagens para \"vetores\" de 28*28 casas ficarem compatíveis\n",
        "      otimizador.zero_grad() # zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # passando as imagens para o modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda do epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() # fazendo a backpropagation a partir da perda\n",
        "\n",
        "      otimizador.step() # atualizando os pesos e bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() # atualizando a perda acumulada\n",
        "\n",
        "    else:\n",
        "      print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "\n",
        "  print(\"\\nTempo de treino (em minutos) =\",(time()-inicio)/60)"
      ],
      "metadata": {
        "id": "zj7daC3j8kjA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizando a validação\n",
        "\n",
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      # desativar o auto_grad para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # saída do modelo em escala logarítmica\n",
        "\n",
        "      ps = torch.exp(logps) # converte a saída do modelo em escala linear\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, no caso, o número que o modelo previu como correto\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred): # compara a previsão com o valor correto\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "\n",
        "  print(\"Total de imagens testadas =\", conta_todas)\n",
        "  print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "psj24B_D-rho"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leitura do modelo\n",
        "\n",
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnqG2WOWAGJa",
        "outputId": "32db9ffd-2cae-4425-a43c-66a1651783a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}